# reference files for sentiment analysis
[sentiment]
## relative directory path to project directory path
ref_dir=references
## replace html tags in raw comments with symbol
html_tag_file = %(ref_dir)s/html_tag_replacement.txt
## 1) replace detailed time (12:01) and amount of money (20 yuan) with symbol 'time' and 'money'
##    based on given regular expression rules
## 2) replace regular cantonese characters or words with mandarin characters or words
entity_mark_file = %(ref_dir)s/entity_mark.txt
## user-defined dictionary for jieba tokenizing
vocab_add_dict_file = %(ref_dir)s/comment_add_dict.txt
## repress to generate words in this dictionary when tokenizing
vocab_del_dict_file = %(ref_dir)s/comment_del_dict.txt
## stop words to be removed during extracting tags
stop_words_file = %(ref_dir)s/chinese_stop_words.txt
## negation words for scoring sentiment strength
negation_word_file = %(ref_dir)s/chinese_negation_words.txt
## sentiment strength and polarity reference csv file following dlut sentiment file format
sentiment_score_file = %(ref_dir)s/sentiment_polarity_strength.csv
## file of enterprise name vs its menu file
branch_store_file = %(ref_dir)s/menu/branch_store_coreference.txt
## theme rule files
process_keyword_rule_file = %(ref_dir)s/take_away_keyword_rule.level3.process_keyword.csv
experience_keyword_rule_file = %(ref_dir)s/take_away_keyword_rule.level3.experience_keyword.csv
regexp_rule_file = %(ref_dir)s/take_away_keyword_rule.level3.regexp.csv
sentiment_rule_file = %(ref_dir)s/take_away_keyword_rule.level3.sentiment.csv
## header of themes from highest level to lowest
theme_header = ['main_group', 'sub_group', 'thd_group']
keyword_header = keyword

# define argument for connect mysql database where to read raw comments and to write analysis results
[database]
localhost = 120.24.62.86
username = root
password = 78iU5478oT0hg
dbname = comments
## word2vec training data tables
w2v_train_tbnames = ['baidu_waimai_comments', 'eleme_comments', 'meituan_comments', 'meituan_waimai_comments', 'nuomi_comments']
w2v_tb_fields =  comment
## table for reading in comment data
readin_tbname = platform_comments
## fields header for extracting comment data
comment_tb = platform_comments
fields = ['comment_id', 'enterprise_id', 'comment', 'comment_time', 'rating', 'platform_id']
comment_id_field = comment_id
enter_field = enterprise_id
comment_field = comment
comment_occur_time_field = comment_time
comment_import_time_field = create_time
rating_field = rating
## enterprise info
enter_tb = enterprise_info
enter_fields = ['enterprise_id', 'enterprise_name']
## table for writing out results data
comment_output_tbname = comments_nlp_results
phrase_output_tbname = comments_phrase_nlp_results
## chunksize for rows each time read in
chunksize = 10000
## table for branch store meal reference
branch_store_tb = branch_store_coreference
## table for theme rule
process_keyword_rule_tb = take_away_rule_process_keyword
experience_keyword_rule_tb = take_away_rule_experience_keyword
regexp_rule_tb = take_away_rule_regexp
sentiment_rule_tb = take_away_rule_sentiment
## entity mark table
entity_mark_tb = entity_mark
## user-defined dictionary
vocab_add_dict_tb = comment_add_dict
vocab_del_dict_tb = comment_del_dict
## stop words table
stop_words_tb = chinese_stop_words
## negation words tb
negation_word_tb = chinese_negation_words
## sentiment strength and polarity reference tb
sentiment_score_tb = sentiment_polarity_strength

# define argument for tokenizing and tags extraction
[tokenizing]
## pos_of_tag (list or None) = If given, only tag whose pos is in the list will be extracted. Otherwise, all tags are accepted.
#pos_of_tag = ['n', 'v', 'a']
pos_of_tag = []
## topk (int or None) = only top k tags will be extracted if provided
comment_topk = 4
phrase_topk = 20
## minimum tf for a token to be treated as a tag
tag_min_tf = 5

# parameters during word2vec model training
[word2vec]
## define dimensionality of features
vocab_dim = 100
## words with minimum word frequency used for word embedding
min_count = 5
## window width - maximum distance between the current and predicted word within a sentence
window = 5
## number of iterations over the corpus
iter = 10

# parameters for lstm classifier training
[lstm]
## maximum number of tokens in a sentences will be used
maxlen = 100
## dropout rate after each layer
dropout = 0.3
## number of epochs
nb_epoch = 20
## batch size during training
batch_size = 250
## activation function to compute output
activation = softmax
## labels during lstm model training and predicting. MUST sort in alphabetic order
lstm_label_header = ['neg', 'pos']
## use generator or not to input training data during training
use_generator = True
## number of samples returned each time by generator
generator_chunksize = 10000

# path to training set for lstm classifier
[model_train]
training_set_path = labeling_training_set
## path to whole comment labeling training set
comment_label_file = %(training_set_path)s/comment_label.part2.csv
## path to phrase comment labeling training set
phrase_label_file = %(training_set_path)s/phrase_label.part2.csv

# path to save lstm model and word2vec model
[model_save]
## path to save model
model_save_path = sentiment/model
## path to save word2vec model
word2vec_comment_model_file = %(model_save_path)s/comment_word2vec.model
word2vec_phrase_model_file = %(model_save_path)s/phrase_word2vec.model
## path to save word2vec vocabulary
word2vec_vocab_file = %(model_save_path)s/word2vec.vocab
## path to save lstm classifier model
comment_lstm_model_file = %(model_save_path)s/comment_lstm_model.h5
phrase_lstm_model_file = %(model_save_path)s/phrase_lstm_model.h5
## path to save vocab tf
tf_file = %(model_save_path)s/tf.txt
comment_idf_file = %(model_save_path)s/comment_idf.txt
phrase_idf_file = %(model_save_path)s/phrase_idf.txt
